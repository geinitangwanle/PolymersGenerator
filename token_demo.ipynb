{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "007cf216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('psmiles_tokenizer_added/tokenizer_config.json',\n",
       " 'psmiles_tokenizer_added/special_tokens_map.json',\n",
       " 'psmiles_tokenizer_added/vocab.json',\n",
       " 'psmiles_tokenizer_added/merges.txt',\n",
       " 'psmiles_tokenizer_added/added_tokens.json',\n",
       " 'psmiles_tokenizer_added/tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model = \"openai-community/gpt2\"  # 你要微调的模型\n",
    "tok = AutoTokenizer.from_pretrained(model)\n",
    "tok.pad_token = tok.eos_token  # 给因果LM一个pad\n",
    "\n",
    "SEP = '|cond|'\n",
    "protected = [\n",
    "    '[*]','Cl','Br','Si','Na','Li','Mg','Ca','Se',\n",
    "    'c','n','o','s','p','C','N','O','S','P','F','I','H',\n",
    "    '=','#','(',')','1','2','3','4','5','6','7','8','9'\n",
    "    '.','/','\\\\','-','@','@@','0',SEP\n",
    "]\n",
    "# 只添加词表中没有的\n",
    "to_add = [t for t in protected if t not in tok.get_vocab()]\n",
    "tok.add_tokens(to_add, special_tokens=False)  # 作为“新增词”，不可再被拆\n",
    "tok.save_pretrained(\"psmiles_tokenizer_added\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df933a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[*]', 'CH', '2', 'CH', '2', '[*]']\n",
      "['c', '1', 'cc', 'cc', 'c', '1']\n",
      "['[*]', 'CH', '2', 'CH', '(', 'Cl', ')', 'CH', '2', '[*]']\n",
      "['target', '_', 'T', 'g', '=', '150', '<', '|cond|', '>']\n"
     ]
    }
   ],
   "source": [
    "def show_tokens(s):\n",
    "    print(tok.tokenize(s))\n",
    "\n",
    "# 连接位点要独立成 token：\n",
    "show_tokens(\"[*]CH2CH2[*]\")           # 期望 ['[*]', 'C', 'H', '2', 'C', 'H', '2', '[*]'] 或近似（byte级仍会把数字独立）\n",
    "# 芳环与环数字\n",
    "show_tokens(\"c1ccccc1\")               # 期望 'c','1','c','c','c','c','c','1'\n",
    "# 卤素不被拆\n",
    "show_tokens(\"[*]CH2CH(Cl)CH2[*]\")     # 期望包含 'Cl' 而不是 'C','l'\n",
    "# 条件分隔\n",
    "show_tokens(\"target_Tg=150<|cond|>\")  # 期望 '<|cond|>' 独立\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad60e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers accelerate datasets tokenizers peft rdkit-pypi\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch, re\n",
    "\n",
    "model = \"openai-community/gpt2\"  # 你要微调的模型\n",
    "tok = AutoTokenizer.from_pretrained(\"psmiles_tokenizer_added\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model)\n",
    "model.resize_token_embeddings(len(tok))\n",
    "\n",
    "def build_example(ex):\n",
    "    cond = ex[\"instruction\"]\n",
    "    out  = ex[\"output\"]\n",
    "    in_ids  = tok(cond + SEP, add_special_tokens=False).input_ids\n",
    "    out_ids = tok(out, add_special_tokens=False).input_ids\n",
    "    ex[\"input_ids\"] = in_ids + out_ids\n",
    "    ex[\"labels\"]    = [-100]*len(in_ids) + out_ids\n",
    "    return ex\n",
    "\n",
    "ds = load_dataset(\"json\", data_files={\"train\":\"train.json\",\"val\":\"val.json\"})\n",
    "ds = ds.map(build_example, remove_columns=ds[\"train\"].column_names)\n",
    "\n",
    "def collate(batch):\n",
    "    ids = [torch.tensor(x[\"input_ids\"]) for x in batch]\n",
    "    labs= [torch.tensor(x[\"labels\"]) for x in batch]\n",
    "    ids  = torch.nn.utils.rnn.pad_sequence(ids,  batch_first=True, padding_value=tok.pad_token_id)\n",
    "    labs = torch.nn.utils.rnn.pad_sequence(labs, batch_first=True, padding_value=-100)\n",
    "    attn = (ids != tok.pad_token_id)\n",
    "    return {\"input_ids\":ids, \"labels\":labs, \"attention_mask\":attn}\n",
    "\n",
    "# ---- 关键 token 加权 Loss ----\n",
    "KT = set(tok.convert_tokens_to_ids(['[*]','=', '#','(',')','1','2','3']))\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):  # 增加 **kwargs\n",
    "        labels = inputs[\"labels\"]\n",
    "        out = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], labels=labels)\n",
    "        logits = out.logits\n",
    "        # shift\n",
    "        sl = labels[:, 1:].contiguous()\n",
    "        lg = logits[:, :-1].contiguous()\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction='none')\n",
    "        loss = loss_fct(lg.view(-1, lg.size(-1)), sl.view(-1))\n",
    "        with torch.no_grad():\n",
    "            key_mask = torch.isin(sl.view(-1), torch.tensor(list(KT), device=sl.device))\n",
    "        loss = torch.where(key_mask, loss*1.8, loss)  # 1.5~2.0 可调\n",
    "        loss = loss.mean()\n",
    "        return (loss, out) if return_outputs else loss\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"psmiles-gpt\",\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_ratio=0.05,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    save_steps=1000, save_total_limit=3,\n",
    "    # fp16=True  # 注释掉或删除这一行\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(model=model, args=args, train_dataset=ds[\"train\"], eval_dataset=ds[\"val\"], data_collator=collate)\n",
    "trainer.train()\n",
    "trainer.save_model(\"psmiles-gpt\")\n",
    "tok.save_pretrained(\"psmiles-gpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de53c9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polymer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
