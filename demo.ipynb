{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c136629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, torch\n",
    "from pathlib import Path\n",
    "repo = Path(\"/Users/tangren/Documents/PolymersGenerator\")\n",
    "sys.path.append(str(repo / \"src\"))  # 允许导入 src 包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96db3b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模块与设备\n",
    "from src.tokenizer import PolyBertTokenizer\n",
    "from src.dataset import make_loader\n",
    "from src.model import VAESmiles\n",
    "from src.train import train_one_epoch, val_loss, set_seed\n",
    "from transformers import AutoModel\n",
    "import torch.optim as optim\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "                      if torch.backends.mps.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294dafc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据与tokenizer\n",
    "csv_path = repo / \"PSMILES_Tg_only.csv\"\n",
    "tokenizer = PolyBertTokenizer(\"kuelumbus/polyBERT\")\n",
    "train_loader = make_loader(\n",
    "    csv_path,\n",
    "    tokenizer,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    col=\"PSMILES\",\n",
    "    max_len=256,\n",
    ")\n",
    "val_loader = make_loader(\n",
    "    csv_path,\n",
    "    tokenizer,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    col=\"PSMILES\",\n",
    "    max_len=256,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ac6dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建带polyBERT编码器的VAE模型\n",
    "polybert = AutoModel.from_pretrained(\"kuelumbus/polyBERT\")\n",
    "model = VAESmiles(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    emb_dim=256,\n",
    "    encoder_hid_dim=polybert.config.hidden_size,\n",
    "    decoder_hid_dim=512,\n",
    "    z_dim=128,\n",
    "    n_layers=1,\n",
    "    pad_id=tokenizer.pad_id,\n",
    "    bos_id=tokenizer.bos_id,\n",
    "    eos_id=tokenizer.eos_id,\n",
    "    drop=0.1,\n",
    "    use_polybert=True,\n",
    "    polybert=polybert,\n",
    "    freeze_polybert=True,\n",
    "    polybert_pooling=\"cls\",\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb2b115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练循环\n",
    "epochs, best = 10, float(\"inf\")\n",
    "for epoch in range(epochs):\n",
    "    kl_w = min(1.0, (epoch + 1) / 10.0)\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer,\n",
    "                                 kl_w, tokenizer.pad_id, device)\n",
    "    val_loss_value = val_loss(model, val_loader, kl_w,\n",
    "                              tokenizer.pad_id, device)\n",
    "    print(f\"[{epoch+1}/{epochs}] train={train_loss:.4f} \"\n",
    "          f\"val={val_loss_value:.4f} kl_w={kl_w:.2f}\")\n",
    "\n",
    "    if val_loss_value + 1e-3 < best:\n",
    "        best = val_loss_value\n",
    "        (repo / \"checkpoints\").mkdir(exist_ok=True)\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"tokenizer_name\": \"kuelumbus/polyBERT\",\n",
    "                \"tokenizer\": tokenizer.get_vocab(),\n",
    "                \"pad_token_id\": tokenizer.pad_id,\n",
    "                \"bos_token_id\": tokenizer.bos_id,\n",
    "                \"eos_token_id\": tokenizer.eos_id,\n",
    "                \"use_polybert\": True,\n",
    "            },\n",
    "            repo / \"checkpoints/notebook.pt\",\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebd6692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成与重构\n",
    "@torch.no_grad()\n",
    "def sample_smiles(model, tokenizer, num=16, max_len=256):\n",
    "    z = torch.randn(num, model.mu.out_features, device=device)\n",
    "    token_ids = model.sample(z, max_len=max_len)\n",
    "    return [tokenizer.decode(row.tolist()) for row in token_ids.cpu()]\n",
    "\n",
    "@torch.no_grad()\n",
    "def reconstruct(model, tokenizer, smiles):\n",
    "    ids = tokenizer.encode(smiles)\n",
    "    enc = torch.tensor(ids, device=device).unsqueeze(0)\n",
    "    mask = (enc != tokenizer.pad_id).long()\n",
    "    mu, logvar = model.encode(enc, mask)\n",
    "    z = model.reparameterize(mu, logvar)\n",
    "    out = model.sample(z, max_len=enc.size(1))\n",
    "    return tokenizer.decode(out.squeeze(0).tolist())\n",
    "\n",
    "model.eval()\n",
    "generated = sample_smiles(model, tokenizer, num=10)\n",
    "recon = reconstruct(model, tokenizer, \"[*]#C[SiH2]C#Cc1cccc(C#[*])c1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a5b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推理用保存的模型\n",
    "ckpt = torch.load(repo / \"checkpoints/notebook.pt\", map_location=device)\n",
    "tokenizer = PolyBertTokenizer(ckpt[\"tokenizer_name\"])\n",
    "polybert = AutoModel.from_pretrained(ckpt[\"tokenizer_name\"])\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.to(device).eval()\n",
    "# 之后可复用 sample_smiles / reconstruct\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
