{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2be0374",
   "metadata": {},
   "source": [
    "## PSMILES 标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "539dc44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from canonicalize_psmiles.canonicalize import canonicalize\n",
    "from rdkit import Chem\n",
    "from typing import Optional\n",
    "import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b36a28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_two_stars(ps): # 确保有且仅有两个连接位点\n",
    "    return ps.count('[*]') == 2\n",
    "\n",
    "def canonicalize_or_skip(psmiles: str) -> Optional[str]:\n",
    "    try:\n",
    "        canon = canonicalize(psmiles)\n",
    "        if not has_two_stars(canon):\n",
    "            return None\n",
    "        # 基础 RDKit 语法校验（PSMILES到分子对象可能需要将 [*] 替换为占位原子）\n",
    "        tmp = canon.replace('[*]', '[Xe]')  # 占位为惰性原子检查价态/语法\n",
    "        if Chem.MolFromSmiles(tmp) is None:\n",
    "            return None\n",
    "        return canon\n",
    "    except Exception:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b62e5804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 CSV、自动识别 PSMILES 列、准备随机种子与工具函数。\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab9885aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using column: PSMILES Total rows: 7367\n"
     ]
    }
   ],
   "source": [
    "CSV_PATH = Path(\"data/PSMILES_Tg_only.csv\")\n",
    "\n",
    "# 载入数据 & 自动识别 PSMILES 列（必要时你可以手工改成具体列名）\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "candidate_cols = [c for c in df.columns if str(c).lower() in (\"PSMILES\")]\n",
    "ps_col = candidate_cols[0] if candidate_cols else df.columns[0]\n",
    "ps_list = df[ps_col].astype(str).tolist()\n",
    "\n",
    "print(\"Using column:\", ps_col, \"Total rows:\", len(ps_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4466df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size = 270\n",
      "mask       = [MASK] 268\n",
      "pad        = [PAD] 267\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "ENC_NAME = \"kuelumbus/polyBERT\"     # polyBERT 自带 SentencePiece 词表\n",
    "tok = AutoTokenizer.from_pretrained(ENC_NAME, use_fast=False)\n",
    "\n",
    "print(\"vocab_size =\", len(tok))\n",
    "print(\"mask       =\", tok.mask_token, tok.mask_token_id)\n",
    "print(\"pad        =\", tok.pad_token, tok.pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bd44b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 是否强制恰好两个 [*]（True/False 可切换）\n",
    "ENFORCE_TWO_STARS = True\n",
    "\n",
    "def simple_clean(ps: str) -> str:\n",
    "    # 仅去空白与极少量全角符号；不做任何 canonicalize 重排\n",
    "    ps = ps.strip()\n",
    "    ps = re.sub(r\"\\s+\", \"\", ps)\n",
    "    ps = ps.replace(\"＃\", \"#\").replace(\"／\", \"/\").replace(\"＊\", \"*\")\n",
    "    return ps\n",
    "\n",
    "def valid_two_stars(ps: str) -> bool:\n",
    "    return ps.count(\"[*]\") == 2 if ENFORCE_TWO_STARS else True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1921953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, numpy as np\n",
    "\n",
    "def span_corrupt_ids(\n",
    "    input_ids,\n",
    "    mask_id,\n",
    "    attention_mask=None,\n",
    "    tokenizer=None,\n",
    "    corruption_ratio=0.2,\n",
    "    mean_span=3,\n",
    "    protect_two_stars=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    更安全的 span corruption：\n",
    "    - 不遮 CLS/SEP/PAD/MASK 等特殊 token\n",
    "    - 只遮 attention_mask==1 的位置\n",
    "    - 默认保护 PSMILES 的聚合位点 token '[*]'\n",
    "    \"\"\"\n",
    "    ids = list(input_ids)\n",
    "    L = len(ids)\n",
    "    if mask_id is None or L == 0:\n",
    "        return ids\n",
    "\n",
    "    # 可遮位置的候选集合\n",
    "    candidates = list(range(L))\n",
    "    if attention_mask is not None:\n",
    "        candidates = [i for i in candidates if attention_mask[i] == 1]\n",
    "\n",
    "    # 保护集合：所有特殊 token（含 CLS/SEP/PAD/MASK 等）\n",
    "    protected = set()\n",
    "    if tokenizer is not None and getattr(tokenizer, \"all_special_ids\", None):\n",
    "        protected.update(tokenizer.all_special_ids)\n",
    "\n",
    "    # 保护 '[*]'（聚合端点）\n",
    "    star_id = None\n",
    "    if protect_two_stars:\n",
    "        try:\n",
    "            star_id = tokenizer.convert_tokens_to_ids(\"[*]\")\n",
    "            if isinstance(star_id, int) and star_id >= 0:\n",
    "                protected.add(star_id)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 去掉受保护 token 的索引\n",
    "    candidates = [i for i in candidates if ids[i] not in protected]\n",
    "    if not candidates:\n",
    "        return ids\n",
    "\n",
    "    # 计算需要遮的数量（基于候选区）\n",
    "    num_to_mask = max(1, int(len(candidates) * corruption_ratio))\n",
    "    covered = 0\n",
    "    used = set()\n",
    "\n",
    "    while covered < num_to_mask and len(used) < len(candidates):\n",
    "        # 随机起点\n",
    "        i = random.choice(candidates)\n",
    "        if i in used:\n",
    "            continue\n",
    "        span = max(1, np.random.poisson(mean_span))\n",
    "        # 尝试向右扩展 span\n",
    "        for k in range(i, min(i + span, L)):\n",
    "            if k in used:\n",
    "                continue\n",
    "            if k not in candidates:\n",
    "                continue\n",
    "            ids[k] = mask_id\n",
    "            used.add(k)\n",
    "            covered += 1\n",
    "            if covered >= num_to_mask:\n",
    "                break\n",
    "\n",
    "    return ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c5768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_span_corrupt(psmiles: str, corruption_ratio=0.2, mean_span=3):\n",
    "    # 分词\n",
    "    enc = tok(psmiles, truncation=True, max_length=128, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    input_ids = enc[\"input_ids\"][0].tolist()\n",
    "    attention_mask = enc[\"attention_mask\"][0].tolist()\n",
    "\n",
    "    # 掩码（调用你写的 span_corrupt_ids 函数）\n",
    "    masked_ids = span_corrupt_ids(\n",
    "        input_ids=input_ids,\n",
    "        mask_id=tok.mask_token_id,\n",
    "        attention_mask=attention_mask,\n",
    "        tokenizer=tok,\n",
    "        corruption_ratio=corruption_ratio,\n",
    "        mean_span=mean_span,\n",
    "        protect_two_stars=True\n",
    "    )\n",
    "\n",
    "    # 转回 token\n",
    "    tokens_original = tok.convert_ids_to_tokens(input_ids)\n",
    "    tokens_masked = tok.convert_ids_to_tokens(masked_ids)\n",
    "\n",
    "    # 打印结果\n",
    "    print(\"原始 PSMILES:\")\n",
    "    print(psmiles)\n",
    "    print(\"\\n原始 tokens:\")\n",
    "    print(tokens_original)\n",
    "    print(\"\\n掩码后 tokens:\")\n",
    "    print(tokens_masked)\n",
    "    print(\"\\n掩码后可读形式:\")\n",
    "    print(tok.convert_tokens_to_string(tokens_masked))\n",
    "\n",
    "\n",
    "# 示例运行\n",
    "test_span_corrupt(\"[*]CC(=O)OCC[*]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32247f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_mask_psmiles(ps_list, corruption_ratio=0.2, mean_span=3):\n",
    "    masked_psmiles = []\n",
    "    for psmiles in notebook_tqdm.tqdm(ps_list, desc=\"Processing PSMILES\"):\n",
    "        enc = tok(psmiles, truncation=True, max_length=128, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        input_ids = enc[\"input_ids\"][0].tolist()\n",
    "        attention_mask = enc[\"attention_mask\"][0].tolist()\n",
    "\n",
    "        masked_ids = span_corrupt_ids(\n",
    "            input_ids=input_ids,\n",
    "            mask_id=tok.mask_token_id,\n",
    "            attention_mask=attention_mask,\n",
    "            tokenizer=tok,\n",
    "            corruption_ratio=corruption_ratio,\n",
    "            mean_span=mean_span,\n",
    "            protect_two_stars=True\n",
    "        )\n",
    "\n",
    "        masked_psmiles.append(tok.convert_tokens_to_string(tok.convert_ids_to_tokens(masked_ids)))\n",
    "    return masked_psmiles\n",
    "\n",
    "# 批量处理\n",
    "masked_ps_list = batch_mask_psmiles(ps_list)\n",
    "\n",
    "# 将结果保存到新的 DataFrame 列中\n",
    "df[\"Masked_PSMILES\"] = masked_ps_list\n",
    "print(df.head())\n",
    "df.to_csv(\"PSMILES_Tg_only_masked.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9063f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "MAX_LEN = 128 # 规定token序列的固定最大长度\n",
    "class PSMILESDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=128):\n",
    "        self.data = dataframe # 包含了原始和掩码后PSMILES的DataFrame\n",
    "        self.tokenizer = tokenizer # 用于分词的tokenizer，这里使用的是polyBERT的tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index] # 获取指定行\n",
    "        masked_psmiles = row[\"Masked_PSMILES\"] # 掩码后的PSMILES\n",
    "        original_psmiles = row[\"PSMILES\"] # 原始PSMILES\n",
    "\n",
    "        # Tokenize masked PSMILES (input to encoder)\n",
    "        masked_enc = self.tokenizer(\n",
    "            masked_psmiles,\n",
    "            truncation=True, # 超过max_len的部分被截断，一般情况下不会\n",
    "            max_length=self.max_len, \n",
    "            padding=\"max_length\", # 填充到固定长度\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Tokenize original PSMILES (target labels)\n",
    "        original_enc = self.tokenizer(\n",
    "            original_psmiles,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = masked_enc[\"input_ids\"].squeeze(0) # 被遮盖的 PSMILES 的 token id 序列，squeeze(0) 去掉批次维度\n",
    "        attention_mask = masked_enc[\"attention_mask\"].squeeze(0) # 1 表示真实 token，0 表示 padding（给模型的自注意力用）\n",
    "        labels = original_enc[\"input_ids\"].squeeze(0) #原始（未遮盖） PSMILES 的 token id 序列，作为目标。\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids, \n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "# 构建数据集\n",
    "dataset = PSMILESDataset(df, tok, max_len=MAX_LEN)\n",
    "\n",
    "# 构建数据加载器\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 示例：查看一个批次的数据\n",
    "for batch in dataloader:\n",
    "    print(\"Input IDs:\", batch[\"input_ids\"].shape)\n",
    "    print(\"Attention Mask:\", batch[\"attention_mask\"].shape)\n",
    "    print(\"Labels:\", batch[\"labels\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567fdf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, BertConfig, BertLMHeadModel, EncoderDecoderModel\n",
    "import torch\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "encoder = AutoModel.from_pretrained(ENC_NAME) #加载预训练的polyBERT模型作为编码器\n",
    "encoder.to(device)\n",
    "\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False # 冻结polyBERT的所有参数\n",
    "\n",
    "hidden_layers = getattr(encoder.config, \"num_hidden_layers\", None)\n",
    "TOP_ENCODER_LAYERS = 2  # # 以后需要更大容量再设为 4\n",
    "\n",
    "def select_encoder_top_layers(model, num_layers=TOP_ENCODER_LAYERS):\n",
    "    \"\"\"这是一个工具函数。\n",
    "    目标：找到 encoder 的最上面 num_layers 层 的参数。\n",
    "    这些参数会被暂时保存起来，将来训练时可以“解冻”它们，让它们恢复 requires_grad=True, 即参与反向传播。\n",
    "    \"\"\"\n",
    "    if hidden_layers is None:\n",
    "        return []\n",
    "    start_idx = max(hidden_layers - num_layers, 0)\n",
    "    prefixes = [f\"encoder.encoder.layer.{idx}.\" for idx in range(start_idx, hidden_layers)]\n",
    "    selected = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(name.startswith(prefix) for prefix in prefixes):\n",
    "            selected.append((name, param))\n",
    "    return selected\n",
    "\n",
    "decoder_config = BertConfig( #基于BERT的解码器配置\n",
    "    vocab_size=len(tok),\n",
    "    hidden_size=512,\n",
    "    intermediate_size=2048,\n",
    "    num_hidden_layers=6,\n",
    "    num_attention_heads=8,\n",
    "    max_position_embeddings=MAX_LEN,\n",
    "    is_decoder=True,\n",
    "    add_cross_attention=True,\n",
    "    pad_token_id=tok.pad_token_id,\n",
    "    bos_token_id=tok.cls_token_id if tok.cls_token_id is not None else tok.pad_token_id,\n",
    "    eos_token_id=tok.sep_token_id if tok.sep_token_id is not None else tok.pad_token_id,\n",
    "    tie_word_embeddings=True,\n",
    ")\n",
    "\n",
    "decoder = BertLMHeadModel(decoder_config)\n",
    "\n",
    "model = EncoderDecoderModel(encoder=encoder, decoder=decoder)\n",
    "model.config.decoder_start_token_id = decoder_config.bos_token_id # decoder的开始token，解码时第一步喂 BOS\n",
    "model.config.pad_token_id = tok.pad_token_id # 对齐 tokenizer\n",
    "model.config.vocab_size = len(tok)  # 输出词表大小\n",
    "\n",
    "model.config.tie_encoder_decoder = False # 共享编码器和解码器的词嵌入\n",
    "model.config.use_cache = False # 训练阶段通常关掉 KV-cache，避免不必要的显存/图不兼容问题\n",
    "model.tie_weights() # 共享编码器和解码器的词嵌入\n",
    "model.to(device)\n",
    "\n",
    "# Keep a handle to the parameters we plan to unfreeze after warmup\n",
    "top_encoder_params = select_encoder_top_layers(model, num_layers=TOP_ENCODER_LAYERS)\n",
    "for name, param in top_encoder_params:\n",
    "    param.requires_grad = False\n",
    "print(f\"Reserved {len(top_encoder_params)} encoder parameter groups for later fine-tuning.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe7f408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "\n",
    "decoder_lr = 2e-4 # 解码器学习率\n",
    "encoder_lr = 2e-5 # 编码器顶部层的学习率\n",
    "NUM_EPOCHS = 5 # 训练轮数\n",
    "GRAD_CLIP_NORM = 1.0 # 全局范数 1.0，防止梯度爆炸，尤其是加入交叉注意力的生成模型\n",
    "UNFREEZE_AFTER_STEPS = 10_000 # 在训练步数达到10,000之后，一次性解冻先前预留的顶层编码器参数\n",
    "\n",
    "# 解码器参数\n",
    "decoder_params = [param for name, param in model.named_parameters() if name.startswith(\"decoder\") or name.startswith(\"lm_head\")]\n",
    "# 编码器顶部层参数\n",
    "encoder_head_params = [param for _, param in top_encoder_params]\n",
    "\n",
    "optimizer_groups = [{\"params\": decoder_params, \"lr\": decoder_lr}] # 组1：解码器（大学习率）\n",
    "if encoder_head_params:\n",
    "    optimizer_groups.append({\"params\": encoder_head_params, \"lr\": encoder_lr}) # 组2：预留的编码器顶层（小学习率）,当训练步数不够时，这组参数不会被更新\n",
    "\n",
    "optimizer = torch.optim.AdamW(optimizer_groups, betas=(0.9, 0.999), weight_decay=0.01) # 带权重衰减的 Adam\n",
    "total_steps = max(len(dataloader) * NUM_EPOCHS, 1) # 总训练步数\n",
    "warmup_steps = int(0.06 * total_steps) # 前 6% 的步数线性升温学习率，之后线性下降到 0\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tok.pad_token_id, label_smoothing=0.1) # 标准自回归语言建模损失，pad_token_id 的标签位置不计入损失\n",
    "encoder_unfreeze_state = {\"done\": False}\n",
    "\n",
    "def maybe_unfreeze_encoder(model, current_step, num_layers=TOP_ENCODER_LAYERS):\n",
    "    # 当 global_step 达到阈值时，一次性把顶层编码器参数解冻。\n",
    "    if encoder_unfreeze_state[\"done\"] or current_step < UNFREEZE_AFTER_STEPS:\n",
    "        return\n",
    "    for name, param in select_encoder_top_layers(model, num_layers):\n",
    "        param.requires_grad = True\n",
    "    encoder_unfreeze_state[\"done\"] = True\n",
    "    print(f\"Unfroze top {num_layers} encoder layers at step {current_step}.\")\n",
    "\n",
    "def train(model, dataloader, num_epochs, start_step=0):\n",
    "    global_step = start_step\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        progress = notebook_tqdm.tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        for batch in progress:\n",
    "            maybe_unfreeze_encoder(model, global_step) # 每个 step 开始前，检查是否要解冻顶层 encoder。\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            decoder_input_ids = model.prepare_decoder_input_ids_from_labels(labels)\n",
    "            decoder_attention_mask = (decoder_input_ids != tok.pad_token_id).long()\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                decoder_attention_mask=decoder_attention_mask,\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            shift_logits = logits[:, :-1].contiguous()\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "            loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            global_step += 1\n",
    "            progress.set_postfix({\"loss\": loss.item(), \"step\": global_step})\n",
    "    return global_step\n",
    "\n",
    "# Example usage (commented out to avoid long runs by default)\n",
    "final_step = train(model, dataloader, num_epochs=NUM_EPOCHS)\n",
    "print(f\"Training finished at step {final_step}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19073d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss, total_tokens = 0.0, 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        dec_in = model.prepare_decoder_input_ids_from_labels(labels)\n",
    "        dec_mask = (dec_in != tok.pad_token_id).long()\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=dec_in,\n",
    "            decoder_attention_mask=dec_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        mask = (labels[:, 1:] != tok.pad_token_id)\n",
    "        total_loss += loss.item() * mask.sum().item()\n",
    "        total_tokens += mask.sum().item()\n",
    "    ppl = torch.exp(torch.tensor(total_loss / max(total_tokens, 1))).item()\n",
    "    return total_loss / max(total_tokens, 1), ppl\n",
    "\n",
    "val_loss, val_ppl = evaluate(model, dataloader)\n",
    "print(f\"val loss/token={val_loss:.4f}, perplexity={val_ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bb5d32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
