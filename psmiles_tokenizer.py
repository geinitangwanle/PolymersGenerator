#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è®­ç»ƒä¸€ä¸ªé¢å‘ P(SMILES) çš„åˆ†è¯å™¨ï¼ˆåŒ–å­¦å•å…ƒä¼˜å…ˆ + BPEï¼‰ã€‚
è¾“å…¥ï¼štrain.json / val.jsonï¼ˆæ¯è¡Œï¼š{"instruction": "...", "output": "..."}ï¼‰
è¾“å‡ºï¼špsmiles_tokenizer/ ç›®å½•ï¼Œå¯è¢« AutoTokenizer ç›´æ¥åŠ è½½ã€‚
"""

import os, json

# =========================
# ğŸ”§ é…ç½®åŒºï¼ˆä½ ä¸»è¦æ”¹è¿™é‡Œï¼‰
# =========================
OUT_DIR = "psmiles_tokenizer"                 # è¾“å‡ºç›®å½•
CORPUS_FILES = ["train.json", "val.json"]     # è®­ç»ƒè¯­æ–™ï¼ˆè¡Œçº§ JSONï¼‰

# è¯è¡¨è§„æ¨¡ä¸è¯é¢‘é˜ˆå€¼
VOCAB_SIZE = 4000
MIN_FREQUENCY = 2

# å¥æœ«è‡ªåŠ¨åŠ  <eos>ï¼ˆå»ºè®® Trueï¼Œæœ‰åŠ©äºæ¨ç†æ—©åœï¼‰
ADD_EOS_AT_END = True

# tokenizers çš„ normalizerï¼ˆä¸è¦ lower/strip accentsï¼Œä¿æŒå¤§å°å†™ï¼‰
USE_NFD_NORMALIZER = True

# special tokens
SPECIAL_TOKENS = ["<pad>", "<eos>", "<bos>", "<unk>"]

# ä½ çš„åˆ†éš”ç¬¦ï¼ˆå’Œè®­ç»ƒ/æ¨ç†ä¿æŒä¸€è‡´ï¼‰
COND_SEP = "|cond|"

# åŒ–å­¦ä¼˜å…ˆåˆ‡åˆ†ï¼ˆæ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨**å­—ç¬¦ä¸²**ï¼Œä¸æ˜¯ re.compileï¼‰
CHEM_RE = (
    r"(" + COND_SEP.replace("|", r"\|") + r")"  # åˆ†éš”ç¬¦ï¼Œè½¬ä¹‰ç«–çº¿
    r"|(\[[^\]\s]+\])"         # æ–¹æ‹¬å·åŸå­ï¼ˆå¦‚ [*], [SiH2]ï¼‰
    r"|(%\d{2})"               # åŒä½ç¯å· %10
    r"|([A-Z][a-z]?)"          # å…ƒç´ ï¼ˆCl, Br, Si, Ge, ...ï¼‰
    r"|([cnospBNOFPSI])"       # èŠ³é¦™/å¸¸è§å…ƒç´ å°å†™
    r"|([=#/\]\\\(\)\-\+\.@])" # é”®ä¸æ§åˆ¶ç¬¦ï¼ˆæ³¨æ„åæ–œæ è½¬ä¹‰ï¼‰
    r"|(\d)"                   # å•ä½ç¯å· 0..9
)

# ä¿å­˜åˆ° tokenizer_config.json çš„é…ç½®ï¼ˆä¾› HF è¯»å–ï¼‰
MODEL_MAX_LENGTH = 2048
PADDING_SIDE = "right"
# =========================


# ============= å®ç°éƒ¨åˆ†ï¼ˆä¸€èˆ¬ä¸ç”¨æ”¹ï¼‰ =============
from tokenizers import Tokenizer, Regex
from tokenizers.normalizers import Sequence, NFD
from tokenizers.pre_tokenizers import Split
from tokenizers.processors import TemplateProcessing
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer


def make_line(ex: dict) -> str:
    """å’Œè®­ç»ƒä¸€è‡´ï¼šinstruction + SEP + output"""
    return f'{ex["instruction"]}{COND_SEP}{ex["output"]}'


def iter_corpus(files):
    for path in files:
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                ex = json.loads(line)
                yield make_line(ex)


def main(USE_UNIGRAM: bool = False):
    os.makedirs(OUT_DIR, exist_ok=True)

    # 1) åˆå§‹åŒ– tokenizer æ¨¡å‹ + è§„èŒƒåŒ– + é¢„åˆ†è¯
    if USE_UNIGRAM:
        raise NotImplementedError("å¦‚éœ€ Unigramï¼Œè¯·æŠŠä¸Šæ–¹ import æ”¹ä¸º Unigram/UnigramTrainer å¹¶åœ¨æ­¤å¤„å®ä¾‹åŒ–ã€‚")
        # tok = Tokenizer(Unigram())
        # trainer = UnigramTrainer(
        #     vocab_size=VOCAB_SIZE,
        #     special_tokens=SPECIAL_TOKENS
        # )
    else:
        tok = Tokenizer(BPE(unk_token="<unk>"))
        trainer = BpeTrainer(
            vocab_size=VOCAB_SIZE,
            min_frequency=MIN_FREQUENCY,
            show_progress=True,
            special_tokens=SPECIAL_TOKENS
        )

    if USE_NFD_NORMALIZER:
        tok.normalizer = Sequence([NFD()])   # è½»åº¦è§„èŒƒï¼›ä¸åš lower/å»é‡éŸ³

    # âœ… ç¨³å®šçš„é¢„åˆ†è¯å™¨ï¼šæŒ‰åŒ–å­¦å•å…ƒåˆ‡åˆ†
    tok.pre_tokenizer = Split(Regex(CHEM_RE), behavior="isolated")

    # 2) è®­ç»ƒ
    tok.train_from_iterator(iter_corpus(CORPUS_FILES), trainer=trainer)

    # 3) å¥æœ«è‡ªåŠ¨åŠ  <eos>
    if ADD_EOS_AT_END:
        tok.post_processor = TemplateProcessing(
            single="$0 <eos>",
            special_tokens=[("<eos>", tok.token_to_id("<eos>"))]
        )

    # 4) ä¿å­˜ä¸º HF å¯è¯»ç›®å½•
    tok.save(os.path.join(OUT_DIR, "tokenizer.json"))
    with open(os.path.join(OUT_DIR, "tokenizer_config.json"), "w", encoding="utf-8") as f:
        json.dump({
            "model_max_length": MODEL_MAX_LENGTH,
            "padding_side": PADDING_SIDE,
            "unk_token": "<unk>",
            "pad_token": "<pad>",
            "eos_token": "<eos>",
            "bos_token": "<bos>"
        }, f, ensure_ascii=False, indent=2)

    print(f"[OK] Tokenizer saved to: {OUT_DIR}/")
    print(f"  - vocab_size (actual): {tok.get_vocab_size()}")

    # ç®€å•å¯è§†åŒ–ä¸€ä¸‹åˆ‡åˆ†
    demo = "[*]C(=O)Cl" + COND_SEP + "[*]C#CC"
    print("Demo pre_tokenize:", tok.pre_tokenizer.pre_tokenize_str(demo))


if __name__ == "__main__":
    # é€šè¿‡å…¥å‚ä¼  flagï¼Œé¿å…ä½œç”¨åŸŸå¯¼è‡´çš„ NameError
    main(USE_UNIGRAM=False)
